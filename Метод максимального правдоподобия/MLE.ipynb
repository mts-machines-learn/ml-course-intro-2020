{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод максимального правдоподобия\n",
    "## Теоретические основы\n",
    "\n",
    "Метод максимального правдоподобия (maximum likelihood estimation - MLE) является одним из наиболее важных как в теоретическом, так и в практическом смыслах методом в машинном обучении. \n",
    "\n",
    "Решение задачи машинного обучения в общих чертах может быть представлено следующей схемой:\n",
    "1. выбирается соответсвующая задаче модель (линейная регрессия, логистическая регрессия и т.п.) $f(X,\\theta)$;\n",
    "2. выбирается функция потерь для оптимизации $L(\\theta,X,y)$;\n",
    "3. выбирается метод оптимизации;\n",
    "4. выбираются данные для обучения $X,y$;\n",
    "\n",
    "После этого обучается модель: рассчитываются минимизирующие функцию потерь параметры $\\theta$ выбранной модели:\n",
    "$$\\theta_{opt} = \\arg \\min L(\\theta,X,y)$$\n",
    "\n",
    "Для простоты и наглядности мы взяли задачу *обучения с учителем*, но подчеркнем, что нет никакой принципиальной трудности рассмотреть и методы без учителя.\n",
    "\n",
    "Можно сказать, что выбор во всех четырех пунктах является до некоторой степени творческой задачей. При этом пункт 2) \"функция потерь\" нужно выделить отдельно: именно функция потерь свидетельствует о хорошем или плохом качестве решения всей задачи машинного обучения.\n",
    "\n",
    "До текущего момента функцию потерь мы выбирали лишь из \"общих соображений\": малое значение суммы квадратов $\\sum_i (y_i - f(X_i,\\theta))^2$ свидетельствует о близости модельных и реальных данных в задаче регрессии, малое значение кросс-энтропии $\\sum_i (y_i\\ln \\sigma(X_i,\\theta) + (1-y_i)\\ln (1-\\sigma(X_i,\\theta))$ свидетельствует о высокой точности меток в задаче классификации. Но есть ли более сильное обоснование выбора функции потерь?\n",
    "\n",
    "Здесь мы поговорим об обоснованиях выбора той или иной функции потерь с точки зрения краеугольных дисциплин для машинного обучения - теории вероятностей и математической статистики.\n",
    "\n",
    "Пусть дан датасет $X$ - признаки и $y$ - разметка или измерение (в дальнейшем будем говорить измерение). Для начала зададим себе вопрос: может ли в i-том примере $(X_i,y_i)$ содержаться некоторая случайность? Ответ на этот вопрос очевиден: да, конечно может и абсолютно точно (исключая тривиальные случаи) содержится. Случайности главным образом проявляются:\n",
    "1. в самих данных $X_i$. Можно сказать, что данные принадлежат некоторому вероятностному распределению $P(X_i)$. Действительно, наличие тех или иных объектов на фото, определенных слов в тексте является по сути случайным, поэтому мы и говорим о распределении $P(X_i)$. Отметим, что распределение $P(X_i)$ может быть как дискретным, так и непрерывным;\n",
    "2. в ошибках измерений $y_i$. Все ошибаются, в том числе и те, кто измеряет (человек или прибор). Основные ошибки измерений происходят в контексте с данными $X_i$. То есть, например, человек принимает собаку за кошку на фотографии не заранее подбрасывая монетку, а по причине того, что кошка на фотографии действительно может сильно напоминать собаку. Это дает нам право ввести условную вероятность $p(y_i|X_i)$. В случае регрессии можно говорить об условной плотности $p(y_i|X_i)$.\n",
    "\n",
    "Сразу отметим, что для практических целей условную вероятность (или плотность) $p(y_i|X_i)$ удобно считать параметрической, то есть $p(y_i|X_i) = p(y_i|X_i,\\theta)$, то есть при изменении параметров $\\theta$ изменяется и $p(y_i|X_i,\\theta)$.\n",
    "\n",
    "Помимо условной вероятности (плотности) i-того примера $p(y_i|X_i,\\theta)$ следует ввести совместную вероятность (плостность) всех примеров $p(y|X,\\theta) = p(y_1,y_2,...,y_i,...,y_n|X,\\theta)$. Если считать каждый i-тый пример независимым от остальных, то совместная вероятность (плотность) немедленно определяется как $p(y|X,\\theta) = \\prod_{i=1}^{n} p(y_i|X_i,\\theta)$\n",
    "\n",
    "После введения совместной условной вероятности (или плотности) $p(y|X,\\theta)$ мы можем непосредственно перейти к определению правдоподобия.\n",
    "\n",
    "Пусть дана совместная вероятность (плотность) $p(y|X,\\theta)$. Если в $p(y|X,\\theta)$ подставить $X,y$ из заданного  датасета, то получится функция $L$, зависящая только от параметров $\\theta: L(\\theta)$. Функция $L(\\theta)$ называется **функцией правдоподобия**. Таким образом, функция правдоподобия при подстановке конкретных параметров $\\theta$ это вероятность (значение плотности вероятности) наблюдать заданные значения $y$ при заданных признаках $X$. Следовательно, при различных параметрах функция правдоподобия может принимать бОльшие или меньшие значения. Мы вплотную подошли к *методу максимального правдоподобия*.\n",
    "\n",
    "*Метод максимального правдоподобия* заключается в поиске таких параметров $\\theta$, которые, собственно, и максимизируют функцию правдоподобия $L(\\theta)$:\n",
    "$$\\theta_{MLE}=\\arg\\max L(\\theta)$$\n",
    "Это кажется очень здравой идеей. Действительно, метод максимального правдоподобия фактически позволяет найти параметры $\\theta$, при которых наблюдать заданный вектор измерений $y$ при заданном $X$ становится наиболее вероятно, что полностью соотносится с концептом машинного обучения в смысле \"объяснения\" признаками $X$ измерений $y$.\n",
    "\n",
    "Для многих практически значимых случаев гораздо лучше оптимизровать не саму функцию правдоподобия $L(\\theta)$, а логарифм от нее $\\ln L(\\theta)$, что не меняет точки максимума $\\theta_{MLE}$, но может существенно облегчить вычисления:\n",
    "$$\\theta_{MLE}=\\arg\\max \\ln L(\\theta)$$\n",
    "\n",
    "Остается вопрос, как быть с новыми данными? Пусть имеется новый объект с признакми $X_{new}$ и полученная в результате применения MLE оценка плотности $p(y_{new}|X_{new},\\theta_{MLE})$. Тогда со многих точек зрения наиболее удачным выбором для оценки $\\tilde{y}_{new}$ будет просто математичсекое ожидание: $\\tilde{y}_{new}=E(y_{new}|X_{new},\\theta_{MLE})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод МНК через MLE\n",
    "\n",
    "Давайте займемся практическим выводом некоторых важнейших функций потерь. Начнем с МНК в линейной регрессии.\n",
    "\n",
    "Заметим, что в задачах регрессии мы имеем дело с плотностями распределений вероятностей.\n",
    "\n",
    "Чтобы продолжить разговор в терминах, введенных выше, давайте поставим задачу линейной регрессии в следующем виде.\n",
    "\n",
    "Пусть даны признаки $X$ и измерения $y$. \n",
    "\n",
    "Чтобы вывести все необходимые условные плотности, сделаем следующее предположение: существует идеальный набор параметров $\\theta_{0}$, для которого выполняется\n",
    "$$\\forall_{i}: y_i = X_i\\theta_0 + \\epsilon_i$$\n",
    "Здесь $\\epsilon_i$ - шум, погрешность, распределенный по нормальному закону $N(0,\\sigma^2)$. Для каждого $i$ и $j$ величины \n",
    "$\\epsilon_i$ и $\\epsilon_j$ считаются **независимыми**.\n",
    "\n",
    "Вас не должно смущать именно такое предположение. Ведь выбирая предварительно линейную модель мы в тайне надеемся, что \"правильный\" ответ в виде $\\theta_{0}$ существует. Шумы считаются нормально распределенными также вполне обоснованно, такая картина часто имеет место быть в различного рода задачах, где присутсвуют ошибки измерения.\n",
    "\n",
    "Наша задача, как и прежде, каким-то образом оценить параметры $\\theta_{0}$.\n",
    "\n",
    "Применим технику максимального правдоподобия.\n",
    "\n",
    "Выражение $y_i = X_i\\theta_0 + \\epsilon_i$ имеет нормальное распределение $N(X_i\\theta_0,\\sigma^2)$. Но так как заранее $\\theta_0$ неизвестно, то мы имеем право предположить, что $y_i$ имеет нормальное распределение $N(X_i\\theta,\\sigma^2)$ с неизвестным $\\theta$. Тем самым мы пришли к знакомой уже плотности $p(y_i|X_i,\\theta) = N(X_i\\theta,\\sigma^2)$. \n",
    "\n",
    "Перейдем к совместной плотности $p(y|X,\\theta) = \\prod_{i=1}^{n} p(y_i|X_i,\\theta)$. По свойству нормального распределения и с учетом езависимости измерений совместное распределение будет являться многомерным нормальным распределением со следующей плотностью:\n",
    "$$ p(y|X,\\theta) = \\dfrac{1}{Z}e^{\\dfrac{-\\sum_i (y_i - X_i\\theta)^{2}}{\\sigma^2}} $$\n",
    "здесь $Z$ не зависит от $X,y,\\theta$\n",
    "\n",
    "Рассматривая совместную плотность как функцию только от параметров $\\theta$, мы получаем функцию правдоподобия:\n",
    "$$L(\\theta) = \\dfrac{1}{Z}e^{\\dfrac{-\\sum_i (y_i - X_i\\theta)^{2}}{\\sigma^2}}$$\n",
    "Теперь нам ничего не мешает перейти к поиску $\\theta_{MLE}$:\n",
    "$$\\theta_{MLE}=\\arg\\max \\ln L(\\theta) = \\arg\\max -\\dfrac{\\sum_i (y_i - X_i\\theta)^{2}}{\\sigma^2}$$\n",
    "Мы избавились от $Z$, так как $Z$ не зависит от параметров $\\theta$.\n",
    "\n",
    "Максимизация функции $f(x)$ эквивалента минимизации $-f(x)$. Тогда оптимальные параметры можно искать как:\n",
    "$$\\theta_{MLE}=\\arg\\min \\dfrac{\\sum_i (y_i - X_i\\theta)^{2}}{\\sigma^2}$$\n",
    "\n",
    "Мы получили уже знакомую задачу нахождения минимума суммы квадратов.\n",
    "\n",
    "Таким образом начальное предположение нормального распределения условной плотности $p(y_i|X_i,\\theta)$ приводит нас к методу наименьших квадратов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод бинарной кросс-энтропии через MLE\n",
    "\n",
    "Регрессия приводит нас к МНК в случае нормального распределения. А что же классификация?\n",
    "\n",
    "Пусть даны признаки $X$ и метки класса $y$. Здесь мы имеем дело уже не с плотностями, а с вероятностями ввиду дискретности $y$.\n",
    "\n",
    "Рассмотрим логистическую регрессию. Как известно для *i*-того примера $\\sigma(X_i\\theta) = \\dfrac{1}{1+e^{X_i\\theta}}$ - это оценка вероятности принадлежности примера к классу $1$. Тогда условную вероятность $p(y_i|X_i,\\theta)$ можно оценить как выражение:\n",
    "$$p(y_i|X_i,\\theta) = \\sigma(X_i\\theta)^y_{i}(1-\\sigma(X_i\\theta))^{1-y_{i}}$$\n",
    "\n",
    "Действительно, если подставить в $p(y_i|X_i,\\theta)$ значение $y_i=1$, то получим $p(1|X_i,\\theta) = \\sigma(X_i\\theta)$, то есть вероятность принадлежности *i*-того примера к классу $1$; если подставить в $p(y_i|X_i,\\theta)$ $y_i=0$, то получим $p(0|X_i,\\theta) = 1-\\sigma(X_i\\theta)$, то есть вероятность принадлежности i-того примера к классу $0$.\n",
    "\n",
    "Перейдем к совместной вероятности $p(y|X,\\theta) = \\prod_{i=1}^{n} p(y_i|X_i,\\theta)$:\n",
    "$$p(y|X,\\theta) = \\prod_{i=1}^{n} \\sigma(X_i\\theta)^y_{i}(1-\\sigma(X_i\\theta))^{1-y_{i}}$$\n",
    "\n",
    "Рассматривая совместную вероятность как функцию только от параметров $\\theta$, мы снова получаем функцию правдоподобия:\n",
    "$$L(\\theta) = \\prod_{i=1}^{n} \\sigma(X_i\\theta)^y_{i}(1-\\sigma(X_i\\theta))^{1-y_{i}}$$\n",
    "И снова нам ничего не мешает перейти к поиску $\\theta_{MLE}$:\n",
    "$$\\theta_{MLE}=\\arg\\max \\ln L(\\theta) = \\arg\\max \\sum_i y_{i}\\ln \\sigma(X_i\\theta) + (1-y_{i})\\ln(1-\\sigma(X_i\\theta))$$\n",
    "\n",
    "Переходим к минимизации:\n",
    "$$\\theta_{MLE}=\\arg\\min \\sum_i -y_{i}\\ln \\sigma(X_i\\theta) - (1-y_{i})\\ln(1-\\sigma(X_i\\theta))$$\n",
    "\n",
    "\n",
    "И вновь мы получили знакомую функцию потерь, на этот раз бинарную кросс-энтропию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
